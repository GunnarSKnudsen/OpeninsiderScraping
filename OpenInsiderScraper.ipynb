{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1990c78c-ea2f-487c-af63-5f4146d3b755",
   "metadata": {},
   "source": [
    "# Script for scraping Openinsider for Directors Dealings\n",
    "\n",
    "Goal of this simple script, is to read in company tickers (and dates), and return all directors dealings of defined types for the period. Data is then cleansed a bit, before being returned as CSVs.\n",
    "\n",
    "This script is written by Gunnar Sjúrðarson Knudsen at TU Wien on 2022-05-27\n",
    "\n",
    "Data is used in relevance to courses 330.214 Project & Enterprise Financing, as well as 194.060 Interdisciplinary Project in collaboration with Professor Aussenegg.\n",
    "\n",
    "\n",
    "### Current Caveats:\n",
    "* Several companies have different ISINs, but the same ticker. as openinsider (to my knowledge) only works with tickers, this might be misrepresenting in certain cases. **Talk to professor of how to handle this**. Possible solution:\n",
    "  * ~~Re-read the given excel files, and figure out which ISIN corresponds to which ticker for a given time-period.~~\n",
    "  * ~~Then split ticker files according to the time periods, and re-save them based on the ISIN (or other identifier)~~\n",
    "  * Nope... That doesn't work, as tickers aren't unique during periods either... Clueless on how to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a97208-c0f3-46c7-abd2-7554ce0cbf8a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c0160-a36a-4cb2-8f88-86a73a5a73fd",
   "metadata": {},
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be33921-e53c-4c31-998f-923900681b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Time Cleaning\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Scraping\n",
    "import requests\n",
    "import locale\n",
    "from pandas.io.json import json_normalize\n",
    "import io\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "# Cleansing\n",
    "import pandas as pd\n",
    "import locale\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41a5d5-980e-41a2-83d9-308d9cc39c03",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd08c97-b273-4f20-be3c-152887a47be4",
   "metadata": {},
   "source": [
    "#### Get relevant tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4ac8fb-de4c-4a2b-8233-17c4ce3fabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tickers_and_isins(_filename):\n",
    "    clear_output(wait=True)\n",
    "    print('Reading tickers')\n",
    "    data = pd.read_excel(_filename, keep_default_na=False, dtype={'TICKER SYMBOL': str})\n",
    "    tickers = data['TICKER SYMBOL'].str.replace(' ', '+')\n",
    "    tickers = tickers.astype(str)\n",
    "    isins = data['ISIN CODE'].str.replace(' ', '+')\n",
    "    isins = isins.astype(str)\n",
    "    \n",
    "    data['TICKER SYMBOL'] = tickers\n",
    "    data['ISIN CODE'] = isins\n",
    "    # Save list somewhere for re-reading?\n",
    "    #display(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c843c-c4af-4959-8c6c-818ef20c092d",
   "metadata": {},
   "source": [
    "#### Define function for downloading a single company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11d8bcd-6c7b-4fc5-806c-6282c8773232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_directors_dealings(_data_location_insider_raw, _ticker, _from_date, _to_date, _dl_type_string, _dl_date_string):\n",
    "    #clear_output(wait=True)\n",
    "    #print('Trying to get directors dealings for ' + _ticker)\n",
    "    f = _data_location_insider_raw + _ticker + '.csv'\n",
    "    if not exists(f):\n",
    "        max_pagination = 100000\n",
    "        insider_data = pd.read_html(f'http://www.openinsider.com/screener?s={_ticker}&o=&pl=&ph=&ll=&lh=&fd=-1{_dl_date_string}&td=0&tdr=&fdlyl=&fdlyh=&daysago={_dl_type_string}&vl=&vh=&ocl=&och=&sic1=-1&sicl=100&sich=9999&grp=0&nfl=&nfh=&nil=&nih=&nol=&noh=&v2l=&v2h=&oc2l=&oc2h=&sortcol=0&cnt={max_pagination}&page=1')\n",
    "        insider_data = insider_data[-3]\n",
    "        insider_data['ticker'] = _ticker\n",
    "        if insider_data.shape[0] >= max_pagination - 1:\n",
    "            print(f\"POSSIBLE ERROR OCCURED HERE for {_ticker}\")\n",
    "\n",
    "        # Rewrite if nothing is found\n",
    "        if insider_data.iloc[0, 0] == 'Sort by':\n",
    "            insider_data = pd.DataFrame(\n",
    "                columns=['X', 'Filing\\xa0Date', 'Trade\\xa0Date', 'Ticker', 'Insider Name', 'Title', 'Trade Type',\n",
    "                         'Price', 'Qty', 'Owned', 'ΔOwn', 'Value', '1d', '1w', '1m', '6m', 'ticker'])\n",
    "        insider_data.to_csv(_data_location_insider_raw + _ticker + '.csv')\n",
    "        print(insider_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23d0da-4a28-49d1-993c-137fb787064c",
   "metadata": {},
   "source": [
    "#### Define loops for tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e710967-e1a2-4d0b-b678-1b18f311b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_directors_dealings(_data_location_insider_raw, _data, _download_type):\n",
    "    # Generate part of URL that defines download type\n",
    "    dl_type_string = ''\n",
    "    for t in _download_type:\n",
    "        dl_type_string = dl_type_string + ('&x' + t.lower() + '=1')\n",
    "    \n",
    "    counter = 0\n",
    "    for index, row in data.iterrows():    \n",
    "        counter = counter + 1\n",
    "        ticker = row['TICKER SYMBOL']\n",
    "        from_date = row['BASE OR ST DATE']\n",
    "        to_date = row['DATE/TIME']\n",
    "        clear_output(wait=True)\n",
    "        print(f'{counter}: Downloading data for ticker {ticker} for period from {from_date} to {to_date}')\n",
    "        \n",
    "        # Generate part of URL that defines period\n",
    "        ## Should be moved to other function - but too late\n",
    "        if from_date == 'NA':\n",
    "            from_date = datetime.datetime.now()\n",
    "        if to_date == 'NA':\n",
    "            to_date = datetime.datetime.now()\n",
    "        dl_date_string = '&fdr=' + (f'{from_date.month:02d}') + '%2F' + (f'{from_date.day:02d}') + '%2F' + (f'{from_date.year:04d}') + '+-+' + (f'{to_date.month:02d}') + '%2F' + (f'{to_date.day:02d}') + '%2F' + (f'{to_date.year:04d}')\n",
    "        \n",
    "        get_single_directors_dealings(_data_location_insider_raw, ticker, from_date, to_date, dl_type_string, dl_date_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241f0de-f0a3-4d4e-8a60-893a01bd447e",
   "metadata": {},
   "source": [
    "#### Define function for cleansing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0d9c2c-f63d-4b46-be64-345e95666371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_directors_dealings(_raw_location, _preprocessed_location):\n",
    "    # List of files to process\n",
    "    filenames = [f for f in listdir(_raw_location) if isfile(join(_raw_location, f))]\n",
    "    for f in filenames:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Processing {f}')\n",
    "\n",
    "        insider_data = pd.read_csv(_raw_location + f, index_col=0)\n",
    "        \n",
    "        # If we got companyName\n",
    "        if insider_data.shape[1] == 18:\n",
    "            insider_data.columns = ['X', 'FilingDate', 'TradeDate', 'Ticker', 'CompanyName', 'InsiderName', 'Title', 'TradeType', 'Price', 'Qty', 'Owned', 'delta_Own', 'Value'\t, '1d'\t, '1w'\t, '1m'\t, '6m'\t, 'ticker']\n",
    "        # And then rename columns\n",
    "        if insider_data.shape[1] == 17:\n",
    "            insider_data.columns = ['X', 'FilingDate', 'TradeDate', 'Ticker', 'InsiderName', 'Title', 'TradeType', 'Price', 'Qty', 'Owned', 'delta_Own', 'Value'\t, '1d'\t, '1w'\t, '1m'\t, '6m'\t, 'ticker']\n",
    "\n",
    "        # Process datatypes accordingly\n",
    "        insider_data['FilingDate'] = pd.to_datetime(insider_data['FilingDate'])\n",
    "        insider_data['TradeDate'] = pd.to_datetime(insider_data['TradeDate'])\n",
    "        ## 'Trade\\xa0Type' - should this be decoded?\n",
    "        insider_data['Price'] = insider_data['Price'].astype(str).map(lambda x: x.replace(',', '').strip('+'))\n",
    "        insider_data['Price'] = insider_data['Price'].map(lambda x: locale.atof(x.strip('$')))\n",
    "        insider_data['Qty'] = pd.to_numeric(\n",
    "            (insider_data['Qty']).astype(str).map(lambda x: x.replace(',', '').strip('+')))\n",
    "        insider_data['Value'] = pd.to_numeric(\n",
    "            insider_data['Value'].map(lambda x: locale.atof(x.replace(',', '').replace('$', ''))))\n",
    "        # insider_data['ΔOwn']=insider_data['ΔOwn'].map(lambda x: locale.atof(x.replace(',', '').replace('%','')))\n",
    "\n",
    "        # Better naming of columns\n",
    "        insider_data.columns = insider_data.columns.map(lambda x: x.replace('\\xa0', '').replace('Δ', 'delta_'))\n",
    "        insider_data.to_csv(_preprocessed_location + f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22498947-8c02-408d-ab60-28fd76ef47a0",
   "metadata": {},
   "source": [
    "## Scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822025e4-89b6-42a9-a75f-25892628af07",
   "metadata": {},
   "source": [
    "### Nasdaq Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8025451-1224-444c-9eb8-76723a828095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VANS.csv\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = 'source/Nasdaq Composite 2004_2022 - total - Knudsen.xlsx'\n",
    "#INPUT_FILE = 'source/SmallTestingFile.xlsx'\n",
    "download_type = ['P', 'S', 'A', 'D', 'G', 'F', 'M', 'X', 'C', 'W']\n",
    "RAW_DATA_LOCATION = 'raw_download/nasdaq/'\n",
    "PROCESSED_DATA_LOCATION = 'processed_data/nasdaq/'\n",
    "\n",
    "# Read in tickers\n",
    "data = read_tickers_and_isins(INPUT_FILE)\n",
    "display(data)\n",
    "\n",
    "# Download the dealings\n",
    "get_all_directors_dealings(RAW_DATA_LOCATION, data, download_type)\n",
    "\n",
    "# Cleanse the dealings\n",
    "preprocess_directors_dealings(RAW_DATA_LOCATION, PROCESSED_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6573e16a-3e60-45c7-ae9d-5f9733fcce70",
   "metadata": {},
   "source": [
    "### S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59fa1601-ebc9-4678-afa8-4d60a6a94183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RCL.csv\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = 'source/S&P 500 - Constituents 1989-2022 - Knudsen.xlsx'\n",
    "#INPUT_FILE = 'source/SmallTestingFile.xlsx'\n",
    "download_type = ['P', 'S', 'A', 'D', 'G', 'F', 'M', 'X', 'C', 'W']\n",
    "RAW_DATA_LOCATION = 'raw_download/sop500/'\n",
    "PROCESSED_DATA_LOCATION = 'processed_data/sop500/'\n",
    "\n",
    "# Read in tickers\n",
    "data = read_tickers_and_isins(INPUT_FILE)\n",
    "display(data)\n",
    "\n",
    "# Download the dealings\n",
    "get_all_directors_dealings(RAW_DATA_LOCATION, data, download_type)\n",
    "\n",
    "# Cleanse the dealings\n",
    "preprocess_directors_dealings(RAW_DATA_LOCATION, PROCESSED_DATA_LOCATION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
